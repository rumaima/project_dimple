<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="DiMPLe introduces a disentangled multi-modal prompt learning method that separates invariant clinical features from spurious cues across text and image modalities for robust generalization under distribution shift.">
  <meta property="og:title" content="DiMPLe: Disentangled Multi-Modal Prompt Learning"/>
  <meta property="og:description" content="Robust adaptation under distribution shift via prompt-guided decoupling of invariant and spurious features in vision-language models."/>
  <meta property="og:url" content="https://openreview.net/forum?id=mYmg0I9jfE"/>
  <meta property="og:image" content="static/image/" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="DiMPLe: Disentangled Multi-Modal Prompt Learning">
  <meta name="twitter:description" content="Domain-robust multi-modal adaptation via disentangled prompt tuning in vision-language models">
  <meta name="twitter:image" content="static/images/dimple_twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="DiMPLe, multi-modal learning, OOD generalization, vision-language models, prompt tuning, disentanglement">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>DiMPLe: Disentangled Multi-Modal Prompt Learning: Enhancing Out-Of-Distribution Alignment with Invariant and Spurious Feature Separation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DiMPLe: Disentangled Multi-Modal Prompt Learning</h1>
          <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                <a href="https://rumaima.github.io/umaimarahman.github.io/" target="_blank">Umaima Rahman</a><sup>1</sup>,</span>
                <span class="author-block">
                <a href="https://mbzuai.ac.ae/study/faculty/mohammad-yaqub/" target="_blank">Mohammad Yaqub</a><sup>1</sup>,</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?user=j5K7HPoAAAAJ&hl=en" target="_blank">Dwarikanath Mahapatra</a><sup>2</sup>,</span>
                </span>
            </div>

            <div class="is-size-5 publication-authors">
                 <span class="author-block"><sup>1</sup>Mohamed Bin Zayed University of Artificial Intelligence</span>
                    <br>
                    &nbsp;
                  <span class="affliatiton"><sup>2</sup>Khalifa Univerity</span>
                  &nbsp;
                    <span class="conference">ICCV 2025</span>  
            </div>
          
          <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href=https://arxiv.org/2506.21237.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>pdf</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.21237" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce DiMPLe (Disentangled Multi-Modal Prompt Learning), a novel method for separating invariant and spurious features across both vision and language modalities. By disentangling multimodal representations while maintaining alignment, DiMPLe enhances robustness to domain shifts and generalization to novel classes. Our method integrates mutual information minimization, spurious feature regularization, and contrastive learning to achieve principled alignment of clinically relevant features. Evaluations across 11 datasets demonstrate substantial improvements over CoOp-OOD, achieving up to 44.31 points gain in novel class accuracy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{rahman2025dimple,
  title={DiMPLe: Disentangled Multi-Modal Prompt Learning},
  author={Anonymous Authors},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template adapted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
